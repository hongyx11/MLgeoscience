# Best practices in the training of Machine Learning models

This lecture is devoted to the training of Machine Learning models in general, with Neural Networks 
representing a subclass of the entire set of models commonly used to learn mappings between some
features and targets. As we will see in the following, a number of `best practices` are in fact 
indipedent on the model used.

## Capacity, overfitting and underfitting
Let's begin by re-stating here the overall aim of a ML model: a model is useful if it can perform
well on new, previously not seen data. This property of a model is also generally referred to as
*generalization*.

In order to be able to assess the generalization capabilities of a model, the dataset available for
training must be divided into 3 distinct sets:

- 
-
-



.....

Generalize the loss functions from LinReg/LogReg, Linreg/Reg:MSE, Logreg/Class:softmax, Class multi see 5.5.1 and 6.2.2.1 and 6.2.2.2, 6.2.2.3 - DONE!!

Pretty much all from Generalization. Follow chapter 5.2 for capacity, over-underfitting, Regularization, hyperparams etc. and also the course

Also need to talk about Performance meausure, TP, FP, Accuracy, Precision etc... look at chapter 5 performance section and nice blog post
https://yanndubs.github.io/machine-learning-glossary/
